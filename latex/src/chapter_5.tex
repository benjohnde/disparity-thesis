\chapter{Evaluation and results}
\label{chap:eval}

%todo smooth me

In the previous chapter, the evaluation suite and its features were presented.
This chapter is split into several subtopics.
First, it deals with the introduction of datasets.
Second, the quality metrics for the comparison of disparity algorithms  for stereoscopic videos are defined.
Third, the structure of the measurement is illustrated.
Fourth, the results are visualized and discussed.
\newline\newline\noindent Overall, the setup for the evaluation took 24 days to process 30 GB of stereoscopic videos and to create all disparity maps.
Accumulated, the computed disparity maps, the created masks as well as the heatmaps, took about 8 GB.

\section{Datasets}

A dataset basically describes a set of stereo images, which additionally contains ground-truth disparity maps.
The aim of such datasets is to provide data which researchers can rely on, to evaluate the performance of computer vision algorithms, such as disparity algorithms in this thesis.
 Without having such datasets it would be crucial to rate the overall quality of for instance stereo matching algorithms.
As for today, no high-resolution stereoscopic video dataset exists yet, neither a synthetic nor a captured one.
In order to obtain ground-truth depth information, there exist in general two options.
On the one hand, the real-world can be sensed via area scanner, for instance a radar.
On the other hand, a synthetic computer-animated scene is generated and the renderer calculates the disparity.
Of course, the former approach is more error-prone than the latter one.
The former one can lack of accuracy due to false measurements whereas the latter one real ground-truth information provides.
\newline\newline\noindent \citeauthor{kondermann2015stereo} came up with an interesting approach.
As area scanners are never 100 percent accurate, they introduced error-bars, which can range from $0-10$.
An error-bar indicates how certain the area scanner was that this disparity for a given pixel really existed \citep{kondermann2015stereo}.
\newline\newline\noindent As it would be a tremendous task to evaluate every existing stereoscopic dataset with every existing disparity algorithms, here three different datasets were chosen.
As reference dataset, the reworked Tsukuba Stereo Dataset was chosen \citep{martull2012realistic}.
The scene is called \textit{Head and Lamp} and was also shortly introduced in the foundations Chapter \ref{chap:foundations}.
\newline\newline\noindent The second one is a dataset, especially created for the evaluation of the DCBGrid from the University of Cambridge\footnote{\url{http://www.cl.cam.ac.uk/research/rainbow/projects/dcbgrid/datasets/}}.
This is also one of the first stereoscopic dataset targeting videos.
As second dataset, a novel, not yet analyzed dataset was chosen: the SVDDD\footnote{SVDDD stands for a high-resolution Stereoscopic Video Dataset with precise Depth and Disparity information.} dataset.
The Lehrstuhl f{\"u}r Praktische Informatik IV\footnote{\url{http://ls.fmi.uni-mannheim.de/de/pi4/}} created the dataset on its own with high-resolution video sequences containing accurate depth and disparity information for stereoscopic videos.
%todo a bit more
Other datasets, which were found but not investigated further more are: MPI Sintel Stereo Training Data, created for optical flow evaluation \citep{Butler:ECCV:2012} and the Middlebury stereo dataset, providing real images with ground-truth information \citep{scharstein2006middlebury}.




\subsection{Tsukuba stereo dataset}

This dataset can be seen as the origin of stereoscopic datasets.
The first dataset ever released for working with stereoscopic images.
\citep{martull2012realistic}.

%todo intro

\subsection{Cambridge stereo dataset}

%todo intro

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{src/images/dbcgrid-dataset.png}
  \caption[Middlebury stereo dataset example]{Middlebury stereo dataset example}
  \label{fig:dbcgrid-dataset}
\end{figure}

The Cambridge stereo dataset consists of five different rendered scenes in $400x300$ resolution with each about 100 frames \citep{richardt2010real}.
The dataset scales the disparity for each scene from $0$ to $64$.

\subsection{SVDDD - a high-resolution Stereoscopic Video Dataset with precise Depth and Disparity information}

The Lehrstuhl f{\"u}r Praktische Informatik IV\footnote{\url{http://ls.fmi.uni-mannheim.de/de/pi4/}} has created a novel dataset on its own containing depth information for stereoscopic videos.
Before the evaluation it was clear that this dataset is going to be evaluated as well.
The difference is that this dataset was not analyzed before.
Thus it is possible that the chosen algorithms work not properly on this dataset.
If this case occurs there exist two possibilities why this happens:

\begin{itemize}
  \item On the one hand, the disparity information are not properly calculated, or
  \item on the other hand, those algorithms have some troubles with the constructed scene.
\end{itemize}

\noindent Focusing on the latter one, the scenes were created with Blender and utilizes the XXX (insert me pls) open-source scenes.
A second camera was added to the scene for obtaining depth information.
The parameter for each scene can be extracted from the paper.
The most important points which can lead to a discrepancy between the computed disparity maps and the ground-truth data are the following:

\begin{itemize}
  \item rays of light were removed only in the ground-truth-data,
  \item motion and object blur was reduced only in the ground-truth data,
  \item fain-grained textures were reduced only in the ground-truth data.
\end{itemize}

%todo describe potential drawbacks
%todo go into more concrete

\section{Quality metrics}

To evaluate the quality of computed disparity maps we need at first to what we compare the computed disparity maps to and secondly how to compare them. In the section \ref{chap:datasets} the availability of such datasets were discussed.

Insert table here.

\begin{table}[h!]
\centering
\begin{tabular}{l|l}
\textbf{Tag} & \textbf{Description} \\ \hline
$\sigma_{10}$ & Guassian noise calculator. \\ \hline
 &  \\ \hline
 &  \\
\end{tabular}
\caption{Overview on used metrics in results}
\label{tab:metrics}
\end{table}

\noindent Typical quality measure instruments for comparing disparity maps against their ground-truth reference data are  \citep{cyganek2011introduction}:

\begin{itemize}
  \item Percentage of bad matching pixels
  \item Root-mean-square error
  \item Parameter-free measures
\end{itemize}

In addition the following are also considered:

\begin{itemize}
  \item Mean absolute error in pixels
  \item Error quantiles
  \item Precision and accuracy
\end{itemize}

\subsection*{Percentage of bad matching pixels}

\begin{equation}
  \operatorname{PBMP}=\frac{1}{n} \sum_{x,y=0}^{}(|d_a(x,y) - d_e(x,y)| > \delta_t)
\end{equation}

\noindent Percentage of bad matching pixels for given threshold.

\subsection*{Root-mean-squared error}

The mean squared error (MSE) as well as the root mean squared error (RMSE) are both the most popular metrics in image and video processing.
The MSE is as the name implies the mean of the squared differences between the intensities of pixels in two pictures at the same position.
In conclusion the average difference per pixel is then the root of the squared error.

\begin{equation}
  \operatorname{RMS-Error}=\sqrt{\frac{1}{n} \sum_{x,y=0}^{}(d_a(x,y) - d_e(x,y))^2}
\end{equation}

\noindent It represents the sample standard deviation of the differences between predicted values and observed values.
Here $d_a(x,y)$ is the actual disparity value for given $x$ and $y$.
$d_e(x,y)$ is our expected disparity value from our ground-truth data.
Hence the RMSE is the difference between values on average.

\section{Measurement}

\begin{itemize}
	\item First took normal depth map images. But not that good, only values from 0-255.
	\item Actual evaluation process consists of comparison of real calculated values of depth map by results of algorithms.
	\item How does the eval actual look like?
\end{itemize} \citep{benoit2008quality}, \citep{scharstein2002taxonomy}.

\subsection*{Parameter tuning}

Our results.

\subsection*{Against reference dataset}

It is also important to have some kind of reference dataset with which the evaluation engine can be calibrated with.
Of course the settings (i.e. parameters of an algorithm) is dependent on the input material (size, noise) and on the scene (e.g. textured vs textureless).
So it is possible to have good parameters for one scene and not for another.
However, in order to evaluate those algorithms the Tsubuka stereo dataset was chosen as reference dataset to see how the eval engine actually works with the same parameters on the same images.

\section{Results}

%todo small intro

The results are visualized with the HSV color model.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.8\textwidth]{src/images/hue-scale.png}
  \caption{Scale of hue of the HSV color model.}
  \label{fig:hue-scale}
\end{figure}

%todo results

\subsection*{Applying disparity algorithms on videos}

As said in the implementation chapter \ref{chap:impl} applying disparity algorithms on videos is trivial and straightforward.
None the less different anomalies can be further investigated while analyzing videos:

\begin{itemize}
  \item outliers in the form of a single frames which differs too much from the others,
  \item impact of noise,
  \item smooth the unknown disparity in frames (next subsection).
\end{itemize}

\noindent As you can see in the following result matrices the quality metrics regarding the salient regions are a bit esoteric. Of course there exist techniques to estimate salient regions in static images and also in videos. In videos moving objects can be detected as salient, referring to motion saliency\citep{opencv_library, wang2014fast}. None the less those bitmasks are a bit more esoteric in the sense of there is room for interpretation.

\begin{landscape}
  \begin{table}[h!]
  \centering
  \begin{tabular}{cll|rrrrr}
    \hline
    \textbf{Rank} & \textbf{Method} & \textbf{Sequence} & \textbf{RMS-All} & \textbf{RMS-Noc} & \textbf{RMS-Sal} & \textbf{RMS-Tex} & \textbf{RMS-DD} \\ \hline \hline
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    2 & StereoBM & SVDDD 02\_rabbit & 4.35\% & 5.43\% & 1.0 px & \cellcolor{red!60}1.1 px & 300ms \\ \hline
  \end{tabular}
  \caption{Result matrix of RMS for videos}
  \label{tab:result-videos-rms}
  \end{table}
\end{landscape}

\begin{landscape}
  \begin{table}[h!]
  \centering
  \begin{tabular}{cll|rrrrr}
    \hline
    \textbf{Rank} & \textbf{Method} & \textbf{Sequence} & \textbf{Out-All} & \textbf{Out-Noc} & \textbf{Out-Sal} & \textbf{Out-Tex} & \textbf{Out-DD} \\ \hline \hline
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px & 1.1 px & 300ms \\
    2 & StereoBM & SVDDD 02\_rabbit & 4.35\% & 5.43\% & 1.0 px & \cellcolor{red!60}1.1 px & 300ms \\ \hline
  \end{tabular}
  \caption{Result matrix of outliers for videos}
  \label{tab:result-videos-out}
  \end{table}
\end{landscape}



\begin{figure}[h!]
\centering
\begin{tabular}{cc}

\subfloat[Left image]{\includegraphics[width=0.47\textwidth]{src/images/svddd-03-left.png}} &
\subfloat[Right image]{\includegraphics[width=0.47\textwidth]{src/images/svddd-03-right.png}} \\

\subfloat[Ground-truth disparity]{\includegraphics[width=0.47\textwidth]{src/images/svddd-03-heatmap-groundtruth.png}} &
\subfloat[SGBM output]{\includegraphics[width=0.47\textwidth]{src/images/svddd-03-heatmap-sgbm.png}} \\

\subfloat[MRF ICM output]{\includegraphics[width=0.47\textwidth]{src/images/svddd-03-heatmap-mrf.png}} &
\subfloat[ELAS output]{\includegraphics[width=0.47\textwidth]{src/images/svddd-03-heatmap-elas.png}} \\

\end{tabular}
\caption{Frame 01, scene 03 rabbit of the SVDDD dataset.}
\label{fig:svddd}
\end{figure}

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{src/images/svddd-03-heatmap-outliers.png}
  \caption{Heatmap of outliers with threshold of 4.0 pixels in computed disparity map with OpenCV SGBM, frame 01, scene 03 rabbit, SVDDD dataset.}
  \label{fig:svddd-07}
\end{figure}

\subsubsection{Result matrix}

Insert overview matrix.

\subsection{Runtime}

Depict:

\begin{itemize}
  \item different performance of disparity algorithms,
  \item down-scaled performance,
  \item first down-scale, then run algorithms, then upscale, runtime,
  \item smoothing-over-time runtime.
\end{itemize}

\subsubsection{Result matrix}

\begin{landscape}
  \begin{table}[h!]
  \centering
  \begin{tabular}{cll|rrr}
    \hline
    \textbf{Rank} & \textbf{Method} & \textbf{Sequence} &  \textbf{Time} & \textbf{Time/MP} & \textbf{Quality index} \\ \hline \hline
    1 & StereoSGBM & SVDDD 02\_rabbit & 4.35\% & \cellcolor{green!60}5.43\% & 1.0 px \\
    2 & StereoBM & SVDDD 02\_rabbit & 4.35\% & 5.43\% & \cellcolor{red!60}1.1 px \\ \hline
  \end{tabular}
  \caption{Result matrix of runtime for videos}
  \label{tab:result-videos-runtime}
  \end{table}
\end{landscape}

\section{Discussion}

This is really important!


