\chapter{Related work}
\label{chap:related}

In this chapter the related work regarding disparity algorithms is treated.
As integration of some disparity algorithms for the later evaluation was part of this thesis, the ones which were actually implemented are examined in more detail.
The well-known semi-global matcher by \citeauthor{hirschmuller2005accurate}, also implemented in the OpenCV library \citep{opencv_library}, is introduced.
OpenCV\footnote{\url{http://opencv.org}} is an extensive image processing framework with the main goal towards real-time computer vision.
\citeauthor{Geiger2010ACCV} introduce an approach that enables fast matching of high-resolution images, which is also outlined in the upcoming section.
Both approaches utilize local methods for estimating disparity maps.
One candidate adopting global methods is the Middlebury MRF library, which is also introduced in this chapter.
It implies solving optimization problems (i.e. the minimization of a global energy cost function).
The libraryâ€™s implemented methods to solve such optimization problems are outlined in greater detail.
In the end, an outlook on spatiotemporal consistency applied on videos is presented.

\section{Semi-global matching}

\citeauthor{hirschmuller2005accurate} combines two different methods, global- and local-matching for determining accurate disparity at a lower runtime as other global algorithms,  which are time consuming even on current hardware \citep{hirschmuller2005accurate, hirschmuller2008stereo}.
\newline\newline\noindent The semi-global matching (SGM) method utilizes pixel-wise matching of so called mutual information (MI) via entropy $H$.
The joint entropy of two images $I_1$ and $I_2$ results from the sum of their combined entropy and a global two-dimensional smoothness constraint $H_{I_1,I_2}$ which leads to the following cost:

\begin{equation}
  MI_{I_1,I_2} = H_{I_1} + H_{I_2} + H_{I_1,I_2}.
\end{equation}

\noindent The discussed one-dimensional constraints from Chapter \ref{chap:foundations} are applied as well.
Calculating the matching cost based on mutual information is insensitive to different video recording conditions and illumination changes \citep{hirschmuller2005accurate, viola1997alignment}.
The joint entropy $H_{I_1,I_2}$ is low (meaning low information content) for rectified images as one image can be predicted by the other.
The MI matching cost is defined as the following:

\begin{equation}
  mi_{I_1,I_2}(i,k) = h_{I_1}(i) + h_{I_2}(k) - h_{I_1,I_2}(i,k),
\end{equation}

\noindent where $h_1$ and $h_2$ are calculated from the probability distribution of corresponding intensities.
Thus, $h_{I_1,I_2}(i,k)$ serves as the matching cost for the two intensities $i$ and $k$.
The idea then is, that one image needs to be warped\footnote{In this context warping can be seen as a function which maps pixels from the destination image to pixels in the original image. Then the pixels are copied at the mapped position to the coordinates in the destination image.} such that corresponding pixels are at the same location in both stereo images:

\begin{equation}
    I_1 = I_b\quad \textrm{and}\quad I_2 = f_D(I_m),
\end{equation}

\noindent where $I_b$ is the base image, $I_m$ the match image and $f_D(x)$ is a function which outputs the matching corresponding point.
As the matching cost represent the information content of two intensities $I_1$ and $I_2$, which should be low (i.e. as equal as possible), the disparity map $D$ needs to be known \textit{a priori} for warping.
Hence, the MI matching cost needs to be calculated either iteratively or hierarchically.
On the one hand, an iteratively approach utilizes a random disparity image for calculating the MI matching cost, which serves as the base for the next iterations.
On the other hand, the MI matching cost can be calculated hierarchically by recursively using an up-scaled disparity image, which has been calculated at half resolution with a common similarity measurement like SAD.
For a deeper explanation of how the mutual information are exactly calculated and used in the SGM method compare \citep{hirschmuller2005accurate, hirschmuller2007evaluation, hirschmuller2008stereo, hirschmuller2011semi}.

\subsection*{OpenCV BM and SGBM}

The OpenCV library \citep{opencv_library}, currently at version 3.1.0, offers two implementations for disparity estimation, block matching and semi-global block matching based on the idea of \citeauthor{hirschmuller2005accurate}.
This version also contains a new filter, which was initially introduced with version 3.0.0, named \textit{Disparity WLS Filter}\footnote{\url{http://docs.opencv.org/3.1.0/d9/d51/classcv_1_1ximgproc_1_1DisparityWLSFilter.html}}.
WLS stands for weighted least squares (in the form of a fast global smoother).
This disparity filter smoothes the disparity and also performs a left-right-consistency check to refine the results in especially half-occluded and uniform areas \citep{min2014fast}.
This yields to better and more accurate results but has the drawback of loosing negative disparity values.
Negative disparity appears if the stereo cameras are verged or inclined towards each other.
The WLS filtering results in disparity ranging from $0$ to $D_{max}$, which is set beforehand as a parameter.
Thus the negative disparity is $-1$.

\section{ELAS: Efficient large-scale stereo matching}

\citeauthor{Geiger2010ACCV} propose a novel approach for estimating the disparity with so called support points \citep{Geiger2010ACCV, Geiger2011IV}.
A support point is like a feature, a point which can be robustly matched.
For those support points, a sparse disparity map is calculated.
For more robustness, only the support points which can be matched left-to-right and right-to-left are retained.
To remove ambiguities, the ratio between the best and the second best match of all points is taken into account.
If the ratio exceeds a fixed threshold, the points are removed.
A support point which has a different disparity value than all its neighbor (adjacent) points, is categorized as an outlier and removed as well.
As the found support points may not cover the whole image, additional support points in the image corners are added.
They adopt the disparity value of their nearest neighbor.
Then, image coordinates of the remaining support points are used to create a 2D mesh via Delaunay triangulation.
To obtain a dense disparity map missing disparities are interpolated using mesh of the Delaunay triangulation by using the nearest-neighbor on the same image line.
For more information how the support points get calculated and how the interpolation is done exactly, compare \citep{Geiger2010ACCV, Geiger2011IV}.

\section{Middlebury MRF library}

The Middlebury MRF library \citep{scharstein2014high, szeliski2008comparative} utilizes a global energy function consisting of Markov random fields to formulate an energy minimization problem and offers the following methods to solve this optimization problem:

\begin{enumerate}
  \item iterated conditional modes (ICM),
  \item graph cuts expansion approach (cf. \citep{boykov2001fast, ramin2004energy, kolmogorov2004energy}),
  \item graph cuts swap approach (cf. \citep{boykov2001fast, ramin2004energy, kolmogorov2004energy}),
  \item sequential tree-reweighted max-product message passing (TRWS)\\(cf. \citep{kolmogorov2006convergent, wainwright2005map}),
  \item sequential belief propagation (BPS) (cf. \citep{boykov2001fast}),
  \item max-product belief propagation (BPM) (cf. \citep{boykov2001fast}).
\end{enumerate}

\noindent The following subsections give a rough overview on some of those methods.
Additionally, a short introduction into MRF-based energy functions is given.
Also, the generalized concepts of how the above techniques help to solve such optimization problems are outlined.

\subsection{Solving optimization problems}

Many problems in computer vision, for instance image smoothing, can be described in terms of energy minimization.
The stereo correspondence problem is formulated in such a way as described in Chapter \ref{chap:foundations}.
Thus, solving of optimization problems is a key part in modern stereo matcher algorithms.
They solve the labelling problem as described in Chapter \ref{chap:foundations}.
Most of the current disparity algorithms use global methods to solve an energy minimization problem.
Usually they utilize Markov random fields (MRF) based energy functions.
As such MRF based energy functions are \textit{NP-hard} approximation algorithms like the following are typically used \citep{tappen2003comparison, cyganek2011introduction}:

\begin{itemize}
  \item dynamic programming,
  \item belief propagation,
  \item graph cuts.
\end{itemize}

\noindent All of these methods have in common that they are supposed to solve so called inference problems, or at least provide approximated solutions.
Markov random fields, mentioned before, are also known as Markov network.
Bayesian networks as well as Markov networks are so called graphical models.
Such graphical models help to understand the reasoning behind those formulations and to actually build algorithms which solve those inference problems.
Both networks express the dependencies of nodes as the conditional probability.
A chain of nodes is called the joint probability\footnote{The joint probability $P(A \land B)$ is the probability of event A and event B occurring. It is the probability of the intersection of two or more events.}.
This then is the product over-all probabilities.
The goal of algorithms which solve inference problem is to compute certain marginal probabilities\footnote{The marginal probability is an unconditional probability as it is not conditioned on another event.}, i.e. the probability that some pixel reach a specific label node \citep{cyganek2011introduction}.
With inference the computation of these marginal probabilities is meant.
Marginal probabilities are defined as the sums over all possible state of all the other nodes in the system.
They are also called beliefs \citep{yedidia2003understanding}.

\subsubsection{Markov random fields}

Markov random fields (MRF), also called Markov network, are used to formulate problems in a probabilistic way represented as an undirected graph consisting of random variables.
For a simple undirected graph compare Figure \ref{fig:simple-graph}.
\newline\newline\noindent A MRF is a graph $G = (V, E)$ where $V = {1,2,...N}$ denotes a set of vertices or nodes.
Each node is associated with a random variable $u_j$ for $j = 1...N$.
$E$ describes the edges $(i,j) \in E$ between the nodes $i$ and $j$.
The neighborhood of a node $i$ is the set of nodes to which the node $i$ is adjacent, i.e. $j \in N$ if and only if $(i,j) \in E$.
The neighborhood of a node $i$ is denoted as $N_i$.

\begin{figure}[h!]
  \centering
  \begin{tikzpicture}
    [scale=1.0,auto=left,every node/.style={circle,fill=blue!18}]
    \node (nA) at (5,9)  {A};
    \node (nB) at (8,9)  {B};
    \node (nC) at (10,7.5) {C};
    \node (nD) at (8,6)  {D};
    \node (nE) at (5,6)  {E};
  
    \foreach \from/\to in {nA/nE,nA/nB,nB/nC,nC/nD,nD/nB,nD/nE}
      \draw (\from) -- (\to);
  \end{tikzpicture}
  \caption[Simple undirected unweighted graph]{Simple undirected unweighted graph}
  \label{fig:simple-graph}
\end{figure}

\noindent The Markov random field satisfies:

\begin{equation}
  P(u_i\ |\ \{u_j\}_{j \in V\\N}) = P(u_i\ |\ \{u_j\}_{j \in N_i}),
\end{equation}

\noindent where $N_i$ is the so called Markov blanket of node $i$.
It describes that the graph should be conditionally independent of all of the other variables given its neighbors.
A hop from one node to another can be seen as a chain of probabilities which have to occur, also called Markov chain.
The main idea behind MRF in combination with computer vision problems is to formulate the labelling problem in such a way, that each pixel has a likelihood to belong to a certain label \citep{tamassia2013handbook}.
The core problem is to find exactly one label for each pixel, which is represented as a node in a MRF.
This label represents the optimal solution to an underlying problem, in the case of stereo correspondence: the disparity of a pixel regarding a reference pixel \citep{cyganek2011introduction}.
\newline\newline\noindent Contrary to MRF, also Bayesian networks exist.
A Bayesian network is a directed graph whereby MRF is undirected.
This implies an important aspect: the direction of a certain probability to hop from one node to another.
Whereby MRF can not represent induced and non-transitive dependencies.
Two independent random variables may be connected by an edge because of possible dependencies.
Bayesian networks overcome these limitations.
\newline\newline\noindent The underlying stereo model of the Middlebury MRF library is based on the research of \citeauthor{sun2003stereo}.
They model stereo matching by three coupled MRF \citep{sun2003stereo}:
\begin{itemize}
  \item $D$ as the smooth disparity field,
  \item $L$ for representing depth-discontinuities,
  \item $O$ is a spatial binary state for handling occlusions.
\end{itemize}
\noindent Figure \ref{fig:mrf-stereo-matching} depicts the relationship between $D$, $L$ and $O$.
The conditional probability\footnote{Bayes' theorem: $P(A|B)$, a conditional probability, is the probability of event A occurring, given that event B occurs. $P(A|B) = \frac{P(B|A)P(A)}{P(B)}$ where $P(A)$ and $P(B)$ are the marginal probabilities of event $A$ and $B$. $P(B|A)$ is the probability of observing event B given that A is true.} over $D$, $L$ and $O$ given a pair of stereo images $I = {I_L,I_R}$ is defined as:
\begin{equation}
  P(D,L,O|I) = \frac{P(I|D,L,O)P(D,L,O)}{P(I)}.
\end{equation}
They then approximate inference via belief propagation over this equation.
For a deeper dive into this topic compare \citep{sun2003stereo, tamassia2013handbook, cyganek2011introduction, yedidia2003understanding, boykov2001fast, kolmogorov2006convergent, wainwright2005map}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.9\textwidth]{src/images/mrf-stereo-matching.png}
  \caption[Stereo matching model by three coupled MRF's]{Stereo matching model by three coupled MRF's \citep{sun2003stereo}.}
  \label{fig:mrf-stereo-matching}
\end{figure}

\subsubsection{Factor graph}

\noindent As those problems are \textit{NP-hard}, there several approximation algorithms exist which are outlined in the following subsections.
All of these approximation algorithms work on factor graphs.
A factor graph represents a factorized function and is bipartite\footnote{If the nodes of a graph can be divided into two disjoint sets, for instance $U$ and $V$, such that every edge connects a node $U$ to one in $V$, then the graph is bipartite.}.
A MRF function is factorized in partial functions and then formulated in a factor graph.
The solution to the problem represented by the factor graph is then approximated.
An important notion of factor graphs is the message which can be passed from a node to another.
There a few exceptions: for instance if the factor graph contains no cycles, the solution can be computed exactly with message passing algorithms (also called belief propagation), which work on a tree.
Otherwise only an approximation is possible.

\subsubsection{Dynamic programming}

In general, dynamic programming means means dividing an optimization problem into smaller chunks.
These chunks get solved individually and in the end, they are connected and the optimization problem is minimized \citep{angel1972dynamic, bellman2015applied, cyganek2011introduction}.
For stereo matching this applies to the partition of a two-dimensional search problem into a series of isolated one-dimensional search problems on each pair of epipolar lines.
These problems are then solved independently.
With dynamic programming the following energy function (introduced in the foundations Chapter \ref{chap:foundations}) can be solved independently per scanline.

\begin{equation}
  E(d) = E_{data}(d) + \lambda E_{smooth}(d)
\end{equation}

\noindent Dynamic programming has two benefits, it is solvable efficiently in polynomial time and it enforces the ordering constraint (as it is solved per scanline).
But it can lead to streaking effects, meaning that the result image seems to be constructed of many independent layers.

\subsubsection{Belief propagation}

Belief propagation (BP) in general is a technique to perform inference on a probabilistic model like Bayesian networks or Markov random fields \citep{yedidia2003understanding, tappen2003comparison, cyganek2011introduction}.
As mentioned before, \citeauthor{sun2003stereo} presented a stereo model for belief propagation.
BP works with messages which are passed from one node to another.
This is the reason why BP is also known as the message-passing algorithm.
The nodes exchange information about probabilities.
In the case of stereo matching, the message contains the probability that the receiver node (a node in MRF) should hold a disparity which is consistent with all information already passed to it by a sender.
The nodes are partitioned into low- and high-confidence ones.
The messages also carry a property, the entropy.
The entropy is high when sending from low- to high-confidence nodes and vice versa (cf. \citep{yedidia2003understanding, tappen2003comparison, cyganek2011introduction}).

\subsubsection{Graph cuts}

For approximating problems formulated in a Markov random field there also exist graph cuts algorithms \citep{boykov2001fast, cyganek2011introduction}.
In general, graph cuts assume a graph $G$ with a set of nodes $N$ and connected by a set of edges $E$.
The goal is to delete enough edges so that each pixel is connected to exactly one label node.
Given a weighted graph $G$ with source $s$ and sink $t$ nodes.
The graph should be partitioned into two sets, $S$ and $T$, where $s \in S$ and $t \in T$.
This set presents a subgraph such that the sum of edge weights spanning this partition is minimized.
\newline\newline\noindent In the case of computer vision, graph cuts are inspired by the combinatorial optimization methods for maximum flow \citep{cyganek2011introduction, cormen2009introduction}.
Two basic variations of the maximum flow problem exist, called $\alpha$-$\beta$-swap and $\alpha$-expansion.
Initially, three labels exist: $\alpha$, $\beta$ and $\lambda$. 
Normally, one step would be to change the label of a pixel, calculate the energy again and then infer if the change was good or not, depending on the delta.
For instance one pixel labelled with $\lambda$ would then be $\beta$.
The $\alpha$-$\beta$-swap algorithm interchanges whole areas of $\alpha$ with $\beta$ whereby areas of $\lambda$ remain unchanged.
In an $\alpha$-expansion a huge number of pixels labelled $\beta$ and $\lambda$ are changed into $\alpha$.
But in each of those methods the outcome is then measured.
In Chapter \ref{chap:foundations} the following equation was introduced:
\begin{equation}
  D = \arg\min_{d} E(d).
\end{equation}
\noindent If the outcome of such a swap or expansion is better, meaning $E(D_{after}) < E(D_{before})$, the algorithm continues.
If not, the algorithm stops.
Thus, both algorithms are expected to be stopped after the first unsuccessful run (i.e. energy increases).
The difficulty is to find the optimal swap move and the initial seed.
Both is described in \citep{boykov2001fast, sinha2004graph, tappen2003comparison, ramin2004energy}.

\section{Spatiotemporal consistency}

Although stereo correspondence is a research field which has been heavily investigated for a few decades, no real disparity algorithm for videos yet exists.
One reason for that can be the lack of solid ground-truth data as only a few datasets has been introduced lately \citep{Butler:ECCV:2012, scharstein2014high}.
However, novel approaches were presented by \citep{richardt2010real}, \citep{khoshabeh2011spatio}, \citep{lee2012local} and \citep{hosni2012temporally}.

%for each approach one paragraph describing what they have done

\noindent \citeauthor{khoshabeh2011spatio}.
\newline\newline\noindent The basic idea behind all of those presented approaches is to handle occurring noise.



\section{Remapping the disparity range of stereoscopic videos}

\citeauthor{lang2010nonlinear} examine the problem of remapping the disparity range of stereoscopic images and video \citep{lang2010nonlinear}.













