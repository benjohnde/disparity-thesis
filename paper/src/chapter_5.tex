\chapter{Evaluation and results}
\label{chap:eval}

In the previous chapter, the evaluation suite and its features were presented.
This chapter is split into several sections.
First, it deals with the introduction of datasets, so which datasets were evaluated and why.
Second, the quality metrics for the comparison of disparity algorithms for stereoscopic videos are defined.
Third, the structure of the measurement is illustrated.
How the measurement were processed and visualized.
Fourth, the results are presented and discussed.
\newline\newline\noindent Overall, the setup for the evaluation took more than 28 days to process about 50 GB of stereoscopic videos and to create all disparity maps.
Accumulated, the computed disparity maps, the created masks as well as the heatmaps, took about 18 GB.

\section{Datasets}

A dataset basically describes a set of stereo images, which additionally contains ground-truth disparity maps.
The aim of such datasets is to provide accurate data, on which researchers can rely.
This datasets can be used to evaluate the performance of computer vision algorithms, such as disparity algorithms in this thesis.
Without having such datasets it would be crucial to rate the overall quality of for instance stereo matching algorithms.
As for today, no high-resolution stereoscopic video dataset yet exists, neither a synthetic nor a captured one.
In order to obtain ground-truth depth information, there exist in general two options.
On the one hand, the real-world can be sensed via area scanner, for instance a radar (cf. KITTI vision benchmark suite\footnote{\url{http://www.cvlibs.net/datasets/kitti/}}).
On the other hand, a synthetic computer-animated scene is generated and the renderer calculates the disparity.
Of course, the former approach is more error-prone than the latter one.
The former one can lack of accuracy due to false measurements whereas the latter one real ground-truth information provides.
\newline\newline\noindent \citeauthor{kondermann2015stereo} came up with an interesting approach.
As area scanners are never 100 percent accurate, they introduced error-bars, which can range from $0-10$.
An error-bar indicates how certain the area scanner was that this disparity for a given pixel really existed \citep{kondermann2015stereo}.
As it would be a tremendous task to evaluate every existing stereoscopic dataset with every existing disparity algorithms, here three different datasets were chosen.
Other datasets, which were found but not investigated are the MPI Sintel Stereo Training Data, created for optical flow evaluation \citep{Butler:ECCV:2012} and the Middlebury stereo dataset, providing real images with ground-truth information \citep{scharstein2006middlebury}.

\subsection*{Tsukuba stereo dataset}

As reference dataset, the reworked Tsukuba Stereo Dataset was chosen \citep{martull2012realistic}.
One of the three scenes is called tsukuba to honour the popular \textit{Head and Lamp} scene and was shortly introduced in the foundations Chapter \ref{chap:foundations}.
The reference dataset is used to see if the implementation leads to similar results as in other stereo benchmarks.
This does of course not verify that the presented implementation is error-free but can point in the right direction.
Of course, the settings (i.e. parameters of an algorithm) depends on the input material (e.g. size of the images, noise occurrences) and on the type of the scenery, for instance many regions with arbitrary surfaces (textured vs textureless).
Hence, it is possible to have good parameters for one scene and not for the other.
However, in order to evaluate those algorithms the reference dataset is used to see how the eval engine actually works with the same parameters on the same images.

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[tsukuba scene]{\includegraphics[width=0.3\textwidth]{src/images/tsukuba-imgL.png}} &
\subfloat[teddy scene]{\includegraphics[width=0.3\textwidth]{src/images/teddy-imgL.png}} &
\subfloat[venus scene]{\includegraphics[width=0.3\textwidth]{src/images/venus-imgL.png}} \\
\subfloat[tsukuba disparity]{\includegraphics[width=0.3\textwidth]{src/images/tsukuba-dgt.png}} &
\subfloat[teddy disparity]{\includegraphics[width=0.3\textwidth]{src/images/teddy-dgt.png}} &
\subfloat[venus disparity]{\includegraphics[width=0.3\textwidth]{src/images/venus-dgt.png}} \\
\end{tabular}
\caption[Tsukuba stereo dataset]{Tsukuba stereo dataset \citep{martull2012realistic}}
\label{fig:tsukuba2}
\end{figure}

\subsection*{Cambridge stereo dataset}

The second one is a dataset, especially created for the evaluation of the DCBGrid from the University of Cambridge\footnote{\url{http://www.cl.cam.ac.uk/research/rainbow/projects/dcbgrid/datasets/}}.
This is also one of the first stereoscopic dataset targeting videos.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{src/images/dbcgrid-dataset.png}
  \caption[Cambridge stereo dataset example]{Cambridge stereo dataset example}
  \label{fig:dbcgrid-dataset}
\end{figure}

\noindent The Cambridge stereo dataset consists of five different rendered scenes in $400x300$ resolution with each about 100 frames \citep{richardt2010real}.
The dataset scales the disparity for each scene from $0$ to $64$.
The disparity maps are delivered as PNG image files, ranging from $0-255$.
Thus, the disparity maps are loaded into a matrix and simply converted to range from $0-64$ by dividing the whole matrix through $4$.
Beforehand, the matrix is converted into $CV_32F$, meaning that each element is represented by 32-bit floating values.

\subsection*{SVDDD - a high-resolution Stereoscopic Video Dataset with precise Depth and Disparity information}

As third dataset, a novel, not yet analyzed dataset was chosen: the SVDDD\footnote{SVDDD stands for a high-resolution Stereoscopic Video Dataset with precise Depth and Disparity information.} dataset.
The department of Praktische Informatik IV\footnote{\url{http://ls.fmi.uni-mannheim.de/de/pi4/}} created the dataset on its own with high-resolution video sequences containing accurate depth and disparity information for stereoscopic videos.
Figure \ref{fig:eval:svddd:intro} depicts the left image and its ground-truth disparity companion for a small selection of the SVDDD dataset.
\newline\newline\noindent The difference here is, that this dataset was not analyzed before.
Thus, it is possible that the chosen algorithms work not properly on this dataset.
If this case occurs there exist two possibilities why this happens.
On the one hand, the disparity information are not properly calculated, or on the other hand, those algorithms have some troubles with the constructed scenery.

\begin{figure}[h!]
\centering
\begin{tabular}{cc}
\subfloat[02-rabbit, left image]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/svddd/02-rabbit-left.png}} &
\subfloat[03-apple, left image]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/svddd/03-apple-left.png}} \\
\subfloat[02-rabbit, disparity]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/svddd/02-rabbit-disparity.png}} &
\subfloat[03-rabbit, disparity]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/svddd/03-apple-disparity.png}}
\end{tabular}
\caption{SVDDD high-resolution stereo dataset}
\label{fig:eval:svddd:intro}
\end{figure}

\noindent Focusing on the latter one, the scenes were created with Blender and utilizes open-source scenes from the Big Buck Bunny project\footnote{\url{https://peach.blender.org}}.
A second camera was added to the scenes for obtaining depth information.
The parameter for each scene can be extracted from the Blender files.
The scenes were also adjusted regarding the following points to extract depth-information accurately.
Rays of light as well as transparent layers were removed to retain accurate depth-information.
Such transparent layers may result in anomalous depth-information as they may bounce back the depth request while obtaining depth information from Blender.
Those transparent layers are in between the actual background of the scene, which is visible for the viewer, and the camera.
Motion and object blur were reduced as such areas can yield to defective disparity information around the blur.
Fain-grained textures like grass or fine hairs were modified.
Without modifying these elements in a scene, a stereo matching algorithm may have problems with those arbitrary textures.
The initial scenes of the Big Buck Bunny project contained randomized grass in each image, left and right side.
This also yields in the impossibility of stereo matching algorithms to determine the shift of the pixels.
\newline\newline\noindent The dataset consists of 15 scenes.
The scenes consist of high resolution stereo images $1920x1080$ with an average bitrate of 94 MBit/s.
With an approximate size of 2 MB for each frame, the size of each sequence varies between 0.4 GB and 2.7 GB.
For each frame, depth and disparity information are computed with 32-bit floating point precision and saved in OpenEXR files.
The above mentioned points apply to the first alpha version of the dataset.
During this thesis, the dataset ran through several iterations as more and more problems arose during the evaluation.
This is discussed later on.

\section{Quality metrics}

Typical quality measure instruments for comparing disparity maps against their ground-truth reference data are  \citep{cyganek2011introduction}:

\begin{itemize}
  \item percentage of bad matching pixels,
  \item root-mean-square error,
  \item parameter-free measures.
\end{itemize}

\noindent As parameter-free measures needs modified disparity algorithms \citep{cyganek2011introduction}, it was not considered.

\subsection*{Percentage of bad matching pixels}

Percentage of bad matching pixels (PBMP) is usually used in comparing the performance of stereo matching algorithms.

\begin{equation}
  \operatorname{PBMP}=\frac{1}{n} \sum_{x,y=0}^{}(|d_a(x,y) - d_e(x,y)| > \delta_t)
\end{equation}

\noindent The threshold, denoted with $\delta_t$, can be adjusted and as result, the percentage of outlier pixels, which differ by more than $\delta_t$, is estimated.

\subsection*{Root-mean-squared error}

The mean squared error (MSE) as well as the root mean squared error (RMSE) are both the most popular metrics in image and video processing \citep{cyganek2011introduction, benoit2008quality, scharstein2002taxonomy}.
The MSE is, as the name implies, the mean of the squared differences between the intensities of pixels in two pictures at the same position.
In conclusion, the average difference per pixel is then the root of the squared error.

\begin{equation}
  \operatorname{RMS-Error}=\sqrt{\frac{1}{n} \sum_{x,y=0}^{}(d_a(x,y) - d_e(x,y))^2}
\end{equation}

\noindent It represents the sample standard deviation of the differences between predicted values and observed values.
Here $d_a(x,y)$ is the actual disparity value for given $x$ and $y$.
$d_e(x,y)$ is our expected disparity value from our ground-truth data.
Hence the RMSE is the difference between values on average.

\section{Measurement}

This chapter summarizes the thoughts behind the evaluation.
All disparity maps were created on a desktop computer with an i5-2500k @ 3.30GHz (quad-core).
First, the idea of a distributed calculation arose, as described in Chapter \ref{chap:impl}.
As this would lack the comparison of runtime, only the desktop computer was used.
\newline\newline\noindent The whole evaluation was proceeded like the following:
Python scripts for the execution of all components in a chain were written.
Disparity maps are computed for each frame in a sequence, for each sequence in a dataset, and for each dataset.
The Cambridge dataset contains five sequences with each 100 frames.
In total 12 algorithms were executed.
This makes an amount of $6.000$ disparity maps.
\newline\newline\noindent In addition, this dataset was duplicated for observing interferences on disparity algorithms.
For analyzing the impact of noise, additive gaussian noise with a normal distribution of $\mathcal{N}(\mu,\sigma^2)$ was added.
For $\sigma$, $10, 20, 30$ were chosen as parameter.
Lower values had no real impact on a few sample disparity maps computations and were also not visually exhibited.
Higher values were analyzed by \citeauthor{richardt2010real} \citep{richardt2010real}, but are clearly disturbing the whole image.
As additive gaussian noise should simulate a more real scenery instead of synthetic rendered videos, higher values were not considered.
\newline\newline\noindent Normally, high resolution videos are not transmitted in their pure nature (lossless), for instance raw files.
That said, H.265 was chosen as current state of the art video compression codec.
H.265 was introduced in Chapter \ref{chap:impl}.
For analyzing the disturbing effects on disparity algorithms, video compression artifacts were added by converting the images to a video, applying the codec and unwrapping the video into images.
As parameter, the constant rate factor (CRF) can be adjusted.
The default parameter value is $28$.
As $0$ means lossless video compression, it was tested with a few sample disparity maps computations.
$0$ had no impact and was not considered for the evaluation.
$51$ was not considered as well, as this totally disturbs the image visually, even if there is no motion between two consecutive frames.
$14, 28, 40$ were chosen as final values to have a feasible amount of different values and also a meaningful jump between those values (always half steps).
\newline\newline\noindent Taking those combinations into account yields to an amount of disparity maps to be computed of $48.000$.
After the disparity maps were computed, other Python scripts were written and executed for the aggregation of the results.
As the results are for each image in a sequence from a dataset and also focusing on one algorithm, they have to be aggregated and written in summarized CSV files.
The simple stereo matcher was treated as an exception.
The reason for this is simple: all the scripts were written with the intent of calculating single independent disparity maps.
As the simple stereo matcher features spatiotemporal consistency, scripts focusing especially on the execution and aggregation of this matcher were written as well.

\subsection*{Parameter tuning}

The observed parameters regarding noise and video compression were described before.
In addition, most of the available parameters were described accurately in Chapter \ref{chap:impl}.
Adjusting and quantifying also the parameters of each algorithm would be a formidable task.
Thus, only the maximum disparity was adjusted according to the dataset.
All the other variables were left as default and moreover, some algorithms have no parameter but the maximum disparity to adjust.

\subsection*{Visualization}

As it may be hard to identify fine-grained changes in grayscale images and also the human eye tends to be more sensitive to observe color changes, heatmaps are used to visualize the results.
For visualization, the heatmaps are created with the OpenCV autumn color scheme.
The color-scale of this theme is depict in Figure \ref{fig:colorscale}.

\begin{figure}[h!]
  \centering
  \includegraphics[width=0.6\textwidth]{src/images/colorscale.png}
  \caption[OpenCV autumn color scale]{OpenCV autumn color scale\protect\footnotemark}
  \label{fig:colorscale}
\end{figure}
\footnotetext{Source (accessed 04/2016): \url{http://docs.opencv.org/3.1.0/d3/d50/group__imgproc__colormap.html}.}

\noindent Outliers are marked with three different purple color steps in the heatmaps.
The darker the purple color is, the more hard the error was.
The light blue means $1px$ threshold was exceeded, the color in-between the light blue and the dark purple denotes $2px$ threshold was exceeded and the very dark purple reflects $4px$ threshold was exceeded.
The border is excluded from the evaluation (cf. Chapter \ref{chap:foundations}) and marked black.
Brighter color means more heat and marks nearer pixels.
An example shot can be seen in Figure \ref{fig:example-heatmap}.
\newline\newline\noindent The maximum disparity in this example shot was $64$.
This means that due to the baseline separation of the cameras, and the resulting shift of the pixels, some pixels can not be calculated as they have no counterpart.
This was excluded from the evaluation.
Also, as the area-based disparity algorithms work with a window size and have a small step from the top and the bottom, this was excluded as well.
Both is represented with a border mask, which was applied to all evaluation steps beforehand.
This can also be seen in Figure \ref{fig:example-heatmap}.
This guarantees a streamlined and balanced evaluation.

\begin{figure}[h!]
\centering
\begin{tabular}{cc}
\subfloat[outliers heatmap]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/street-example-heatmap-outliers.png}} &
\subfloat[ground-truth heatmap]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/street-example-heatmap.png}}
\end{tabular}
\caption{Example heatmaps for outliers heatmap and disparity ground-truth.}
\label{fig:example-heatmap}
\end{figure}

\subsection*{Applying disparity algorithms on videos}

As said in the implementation Chapter \ref{chap:impl}, disparity algorithms for image can be applied to videos.
Although the frames of a video can be seen as independent pieces, different anomalies can be further investigated while analyzing videos:

\begin{itemize}
  \item outliers in the form of a single frames which differs too much from the others,
  \item impact of additive Gaussian noise to simulate real scenery,
  \item impact of disturbing effects like artifacts from video compression,
  \item the benefit from creating a naive spatiotemporal context.
\end{itemize}

\noindent The following sections present only the highlights of the results as it is an tremendous amount of data, which were computed during the evaluation.

\section{Results}

The following subsections present the results of the evaluation.
The highlights are illustrated as the amount of information, which were obtained during the measurement is huge.
The following Table \ref{tab:identifier1} contains the identifier which are used throughout the upcoming subsections.
The following subsections are rationed in the results against the reference dataset, the general performance of the Cambridge dataset as well as the impact of interferences like noise and video compression.
Afterwards, the runtime and the SVDDD dataset are outlined.

\begin{table}[h!]
\centering
\begin{tabular}{ll}
  \hline
  \textbf{Id} & \textbf{Sequence} \\ \hline \hline
  1 & book \\
  2 & street \\
  3 & tank \\
  4 & temple \\
  5 & tunnel \\
  & \\
  & \\
  & \\
  & \\
  & \\
  & \\
  & \\
  & \\
  \hline
\end{tabular}
\quad
\begin{tabular}{ll}
  \hline
  \textbf{Id} & \textbf{Algorithm} \\ \hline \hline
  1 & OpenCV - SGBM \\
  2 & OpenCV - BM \\
  3 & ELAS \\
  4 & MRF - ICM \\
  5 & MRF - GC Expansion \\
  6 & MRF - GC Swap \\
  7 & MRF - BP TRWS \\
  8 & MRF - BP BPS \\
  9 & MRF - BP BPM \\
  10 & OpenCV - Simple BM \\
  11 & SNSM \\
  12 & SNSM - STU\footnote{Simple stereo matcher with respecting spatiotemporal context, unweighted.} \\
  13 & SNSM - STW\footnote{Simple stereo matcher with respecting spatiotemporal context, weighted.} \\
  \hline
\end{tabular}
\caption{Identifier for results}
\label{tab:identifier1}
\end{table}

\noindent The general procedure is to describe the high level results of algorithms operate on a whole sequence with PBMP$_{1px}$ utilizing the non-occluded mask.
As the focus is on PBMP$_{1px}$, lower results are better.
Then, the other masks are outlined in greater detail.
Afterwards, particular outliers regarding the whole sequence are discussed.
The same procedure is then applied to impact of noise and impact of video compression.

\subsection{Against reference dataset}

As reference scene the \textit{Head and lamp} scene of the Tsukuba dataset was chosen.
For evaluating against the reference dataset, all presented disparity algorithms computed disparity maps.
These disparity maps are then compared in combination with different masks.
The result can be seen in Table \ref{tab:eval:snsm:tsukuba} and the SNSM is depict in Figure \ref{fig:eval:snsm:tsukuba}.
Compared with the Middlebury stereo benchmark\footnote{\url{http://vision.middlebury.edu/stereo/eval/}}, similar results could be achieved.
The SNSM utilizes SAD as matching cost function and does not further disparity refinement.
The results from SNSM with a threshold of $4$ and a block size of $9$ are $\delta_{all} = 6.78\%$ and $\varepsilon_{all} = 2.03px$.
According to the Middlebury stereo benchmark, the other SAD implementation (SAD-IGMCT) \citep{ambrosch2010accurate} achieved with a threshold of $2$: $\delta_{all} = 2.92\%$.
The difference comes from smoothing, which SAD-IGMCT included as a disparity refinement step, which is visible while looking at their outcome.

%todo add other algorithm outputs

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[ground-truth heatmap]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/snsm-tsukuba-ground-truth.png}} &
\subfloat[computed disparity-map]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/snsm-tsukuba-disparity-map.png}} \\
\subfloat[outliers in SNSM]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/snsm-tsukuba-outliers.png}} &
\subfloat[outliers in SAD-IGMCT]{\includegraphics[width=0.45\textwidth]{src/images/evaluation/snsm-tsukuba-outliers-igmct.png}}
\end{tabular}
\caption{SNSM heatmaps for Tsukuba scene}
\label{fig:eval:snsm:tsukuba}
\end{figure}

\begin{table}[h!]
\centering

\begin{tabular}{ll|l|l|}
\cline{3-4}
 &  & \multicolumn{2}{l|}{Metrics} \\ \cline{3-4} 
 &  & $\varepsilon_{all}$ & $\delta_{all}$ \\ \hline
\multicolumn{1}{|l|}{\multirow{3}{*}{Algorithm}} & SAD & $6.78\%$ & $2.03px$ \\ \cline{2-4} 
\multicolumn{1}{|l|}{} & SAD-IGMCT\footnote{Bla.} & $2.93\%$ & $1.38px$ \\ \cline{2-4} 
\multicolumn{1}{|l|}{} & - & - & - \\ \hline
\end{tabular}
\caption{Result table for reference dataset}
\label{tab:eval:snsm:tsukuba}
\end{table}

\subsection{General performance}

%todo small intro

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Frame 8]{\includegraphics[width=0.3\textwidth]{src/images/evaluation/outliers/image0008-heatmap-outliers.png}} &
\subfloat[Frame 23]{\includegraphics[width=0.3\textwidth]{src/images/evaluation/outliers/image0023-heatmap-outliers.png}} &
\subfloat[Frame 27]{\includegraphics[width=0.3\textwidth]{src/images/evaluation/outliers/image0027-heatmap-outliers.png}}
\end{tabular}
\caption{Outliers observing three frames in 01-book sequence of Cambridge dataset, calculated with StereoBM.}
\label{fig:outliers1}
\end{figure}

\noindent As learned in Chapter \ref{chap:foundations}, it is difficult for disparity algorithms to clearly treat object borders at which depth-discontinuity occurs.

\begin{figure}[h!]
  \centering
  \includegraphics[width=1.0\textwidth]{src/images/evaluation/outliers/plot1.pdf}
  \caption{$\varepsilon_{disc}$ for 01-book sequence of Cambridge dataset, calculated with StereoBM.}
  \label{fig:plot1}
\end{figure}

%todo explain algorithms numbers
%todo explain sequences numbers

\begin{table}[]
\centering
\begin{tabular}{c|ccccccccc}
  & 1 & 2 & 3 & 4 & 5 & 6 & 7 & 8 & 9 \\ \hline
1 & 30.89 & 30.81 & 26.88 & 26.88 & 26.88 & 26.88 & 26.88 & 26.88 & 26.88 \\
2 &   &   &   &   &   &   &   &   &  \\
3 &   &   &   &   &   &   &   &   &  \\
4 &   &   &   &   &   &   &   &   &  \\
5 &   &   &   &   &   &   &   &   &  \\ \hline
$\varnothing$ &   &   &   &   &   &   &   &   & 
\end{tabular}
\caption[Result table for general performance]{Result table for general performance, focusing on PBMP$_{1px}$}
\end{table}

\begin{table}[]
\centering
\begin{tabular}{c|cccc}
  & 10 & 11 & 12 & 13 \\ \hline
1 & 30.89 & 30.81 & 26.88 & 26.88 \\
2 &   &   &   &  \\
3 &   &   &   &  \\
4 &   &   &   &  \\
5 &   &   &   &  \\ \hline
$\varnothing$ &   &   &   & 
\end{tabular}
\caption[Result table for general performance]{Result table for general performance, focusing on PBMP$_{1px}$}
\end{table}

\subsection{Impact of noise}

The impact of noise is an interesting and not often evaluated approach regarding disparity algorithms.
%todo more intro

\subsection{Impact of video compression}

The impact of video compression is an interesting and novel approach.
High-resolution videos, due to its nature, are not shipped for television as uncompressed raw data or even lossless compressed.
As seen in the Chapter \ref{chap:related}, there are different applications for disparity algorithms.
One outpointed need is the remapping of disparity for different screen sizes.
If this can not happen on raw data, for instance the data is missing and only the compressed version is available, the influence of video compression on the outcome of disparity algorithms is observed.
\newline\newline\noindent For this observation five different datasets were created with the image diminisher.
As underlying video codec H.265 was chosen because it is currently the most efficient video compression technique.
The recommended default parameter for H.265, the constant rate factor (CRF) is $28$.
It ranges from $0-51$, whereas $0$ represents lossless and $51$ the worst possible compression.
$0, 14, 28, 40, 51$ were chosen with $14$ and $40$ as additionally in-between parameters.
Thus, the observed range is $0, 14, 28, 40, 51$.

\subsection{Runtime}

%todo small intro

\subsection{Simple stereo matcher}

%todo small intro

\subsection{SVDDD}

The SVDDD dataset from the department of Praktische Informatik IV\footnote{\url{http://ls.fmi.uni-mannheim.de/de/pi4/}} was evaluated as well.
During this thesis, the dataset ran through several iterations as more and more problems arose during the evaluation.
In addition, some algorithms were not able to process high-resolution stereo images.
Thus, it is separated from the other results.
\newline\newline\noindent The initially evaluated scenes led to strange results.
Some areas were computed correctly but some areas were completely off.
One major point, which can lead to such strange, defective results, is negative disparity.
Most of the algorithms expect that disparity values range from $0-d_{max}$.
Even a small amount of negative disparity can conduct to a result, which is slightly off, i.e. three or four pixels.
Looking at PBMP$_{1px}$ , which can yield to nearly $50\%$.
An example can be seen in Figure \ref{fig:outliers-rabbit-svddd}, illustrating two images.
In the left image about $10\%$ pixels overall contains slightly negative disparity whereas in the right image, the disparity ranges from $0-59$.

\begin{figure}[h!]
\centering
\begin{tabular}{ccc}
\subfloat[Negative disparity]{\includegraphics[width=0.48\textwidth]{src/images/evaluation/02-rabbit-neg-elas.png}} &
\subfloat[Only positive disparity]{\includegraphics[width=0.48\textwidth]{src/images/evaluation/02-rabbit-elas.png}} &
\end{tabular}
\caption[Comparison of computed disparity maps regarding negative disparity]{Comparison of computed disparity maps regarding negative disparity.}
\label{fig:outliers-rabbit-svddd}
\end{figure}

\noindent Another challenge of high-resolution datasets is the huge computational power which is necessary to compute disparity maps for them.
The BP algorithms would have needed about $52 GB$ of memory to calculate the scenes for $d_{max} = 64$ as the labels for all possible states have to be created and stored in memory.
Also, the SNSM wanted to allocate about $35 GB$ of memory as the disparity space image is computed \textit{a priori} before the actual calculation of the disparity maps takes place.
The runtime also gives a feeling how computational powerful and complex it is, to perform a disparity estimation on high-resolution stereo images.
Summing up, the MRF BP algorithms as well as the SNSM were excluded from the evaluation of the SVDDD dataset.
\newline\newline\noindent Table XYZ contains the mean results of the SVDDD dataset.
The best result is achieved with the MRF GC Expansion algorithm, which only led to $0.65\%$ of bad matching pixels in the rabbit scene.
OpenCV SGBM and ELAS also lead to good results.
Comparing the runtime of such high-resolution scenes, the OpenCV SGBM clearly outperforms the others.
Taking the general performance into account, OpenCV SGBM and ELAS both yield to feasible results.





\section{Discussion}

%todo write about simple stereo matcher in comparison to opencv implementations



The results regarding Gaussian noise differ from the results, which \citeauthor{richardt2010real} \citep{richardt2010real} achieved.
Here, a much lower $\sigma^2$ led to defective results.
As every pixel gets changed according to the normal distribution and additionally, for each image a different noise matrix was calculated to avoid pattern matching, the stereo image gets disturbed in an unnatural way.
\citeauthor{richardt2010real} added additive Gaussian noise to simulate a more natural, real scenery.
Gaussian noise is a part of sensor noise, but a small one.
To simulate such real scenery, a more realistic model should be created.
For the purpose of simulating an image, captured via CCD sensor, camera noise models exist \citep{liu2006noise}.
Such complex noise models are difficult to create, but maybe lead to much more feasible results.




%todo write a bit about video compression



%todo write about runtime in comparison with results



















